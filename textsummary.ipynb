import google.generativeai as genai
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# 🔹 Step 1: Setup Google Gemini API
API_KEY = input("Enter your Google Gemini API Key: ")  # User enters API key
genai.configure(api_key=API_KEY)

# List available models
models = genai.list_models()
available_models = [model.name for model in models]
print("\n✅ Available Models:", available_models)

# Choose the best available model dynamically
MODEL_NAME = "models/gemini-1.5-pro"  # Default model

if "models/gemini-2.5-pro-exp-03-25" in available_models:
    MODEL_NAME = "models/gemini-2.5-pro-exp-03-25"
elif "models/gemini-2.0-pro-exp" in available_models:
    MODEL_NAME = "models/gemini-2.0-pro-exp"

print("✅ Using Model:", MODEL_NAME)

# 🔹 Step 2: Define Text Summarization Function Using Gemini
def summarize_text(text):
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(f"Summarize this text:\n{text}")
        return response.text
    except Exception as e:
        print("❌ Error generating summary:", e)
        return "Summarization failed."

# 🔹 Step 3: Granular Processing (Split into Sentences)
def split_into_sentences(text):
    return text.split(". ")  # Simple sentence segmentation

# 🔹 Step 4: Convert Sentences to Feature Vectors (TF-IDF)
def vectorize_sentences(sentences):
    vectorizer = TfidfVectorizer(stop_words="english")
    return vectorizer.fit_transform(sentences)

# 🔹 Step 5: Cluster Sentences into Granules Using K-Means
def cluster_sentences(sentences, num_granules=5):
    if len(sentences) < num_granules:
        num_granules = len(sentences)  # Adjust clusters to avoid errors

    X = vectorize_sentences(sentences)
    kmeans = KMeans(n_clusters=num_granules, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)

    granules = {i: [] for i in range(num_granules)}
    for i, label in enumerate(labels):
        granules[label].append(sentences[i])

    return granules

# 🔹 Step 6: Summarize Each Granule Using Gemini
def summarize_granules(granules):
    summaries = []
    for granule in granules.values():
        combined_text = " ".join(granule)  # Merge sentences in each granule
        summary = summarize_text(combined_text)  # AI summarization
        summaries.append(summary)
    return summaries

# 🔹 Step 7: Final Summarization (Merging Granule Summaries)
def final_summary(summaries):
    return summarize_text(" ".join(summaries))

# 🔹 Step 8: Execute Summarization Pipeline
def summarize_large_text(text, num_granules=5):
    sentences = split_into_sentences(text)
    granules = cluster_sentences(sentences, num_granules)
    granule_summaries = summarize_granules(granules)
    final_summarized_text = final_summary(granule_summaries)

    return final_summarized_text

# 🔹 Step 9: Get Input from User
def get_user_input():
    print("\nChoose Input Method:")
    print("1. Enter text manually")
    print("2. Upload a text file")

    choice = input("Enter your choice (1 or 2): ").strip()

    if choice == "1":
        text = input("\nEnter the text to summarize:\n")
    elif choice == "2":
        file_path = input("\nEnter the file path: ").strip()
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                text = file.read()
            print("\n✅ File loaded successfully!")
        except FileNotFoundError:
            print("❌ Error: File not found.")
            return None
    else:
        print("❌ Invalid choice. Please enter 1 or 2.")
        return None

    return text

# 🔹 Step 10: Run Summarization
if __name__ == "__main__":
    user_text = get_user_input()

    if user_text:
        num_granules = int(input("\nEnter the number of granules (recommended: 3-5): "))
        summary = summarize_large_text(user_text, num_granules)
        print("\n🔹 Final Summarized Text:\n", summary)
