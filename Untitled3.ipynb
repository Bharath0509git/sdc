import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.datasets import mnist

# Step 1: Load MNIST Dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Flatten images for clustering (convert 28x28 to 784)
x_train_flat = x_train.reshape(-1, 28*28).astype("float32") / 255.0
x_test_flat = x_test.reshape(-1, 28*28).astype("float32") / 255.0

# Normalize data
scaler = StandardScaler()
x_train_flat = scaler.fit_transform(x_train_flat)
x_test_flat = scaler.transform(x_test_flat)

# Step 2: Apply Granular Clustering (K-Means)
granule_size = 2000  # Number of points per granule
num_granules = max(1, len(x_train_flat) // granule_size)

kmeans = KMeans(n_clusters=num_granules, random_state=42)
granule_labels = kmeans.fit_predict(x_train_flat)

# Step 3: Transform Data Using Granule Centroids
granule_centers = kmeans.cluster_centers_
x_train_transformed = np.array([granule_centers[label] for label in granule_labels])

# Repeat for test set
test_granule_labels = kmeans.predict(x_test_flat)
x_test_transformed = np.array([granule_centers[label] for label in test_granule_labels])

# Step 4: Define a Simple Neural Network
model = keras.Sequential([
    keras.layers.Dense(128, activation="relu", input_shape=(x_train_transformed.shape[1],)),
    keras.layers.Dense(64, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])

# Compile Model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# Step 5: Train the Model
model.fit(x_train_transformed, y_train, epochs=10, batch_size=32, validation_data=(x_test_transformed, y_test))

# Step 6: Evaluate Model
test_loss, test_acc = model.evaluate(x_test_transformed, y_test)
print(f"Test Accuracy: {test_acc:.4f}")
